{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar IF3170 Inteligensia Buatan 2017/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplikasi Web Prediksi Income Per Tahun Tahap 1 : Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anggota :\n",
    "    1. Dewita Sonya Tarabunga - 13515021\n",
    "    2. Helena Suzane Graciella - 13515032\n",
    "    4. Emilia Razak - 13515056\n",
    "    3. Jehian Norman Saviero - 13515139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data training yang diberikan merupakan data dari 32561 hasil sensus yang memberikan data-data berupa age, workclass, final weight, education, education number, marital status, occupation, race, sex, capital gain, capital loss, hours per week, dan native country. Terdapat juga data test yang akan digunakan untuk melakukan test hasil pembelajaran.\n",
    "\n",
    "Analisis yang kami lakukan terhadap data adalah analisis terhadap missing value pada data. Terdapat 3 kolom pada data yang memiliki missing value, yaitu '?' yaitu workclass, occupation, dan native country. Selain itu, semua kolom lain tidak memiliki missing value. Setelah dilihat dari data, semua orang yang memiliki missing value pada occupation hanya memiliki dua kemungkinan nilai workclass, yaitu Never-worked atau missing value. Jelas bahwa jika workclass adalah Never-worked, maka orang tersebut tidak memiliki pekerjaan sehingga value nya kosong. Lalu, jika dilihat dari data yang memiliki missing value di keduanya, hampir 90% dari orang tersebut memiliki gross income <= 50K, maka kami mengasumsikan bahwa orang tersebut pernah bekerja, namun sekarang tidak bekerja sehingga tidak masuk ke kategori manapun dalam workclass. Maka kami mengubah missing value di workclass menjadi Not-worked dan missing value di occupation menjadi None.\n",
    "\n",
    "Satu kolom lagi yang memiliki missing value adalah native country, namun native country merupakan data categorical sehingga tidak masuk akal jika diganti dengan data mean atau median, begitu pula data modus juga kurang cocok untuk menggantikan missing value. Maka kami menganggap missing value sebagai negara lain di luar daftar, atau Other.\n",
    "\n",
    "Setelah analisis, kami akan melakukan eksperimen dengan akurasi menjadi ukuran kinerja. Kami akan memilih algoritma dengan akurasi terbaik untuk melakukan full traning dan menghasilkan model untuk kemudian digunakan dalam memrediksi data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperimen : 10-Cross Fold Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca data training dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age   workclass  fnlwgt        edu  edunum             marital occupation  \\\n",
      "0   42  Not-worked   51795    HS-grad       9            Divorced       None   \n",
      "1   60  Not-worked  251572    HS-grad       9             Widowed       None   \n",
      "2   28  Not-worked  201844    HS-grad       9           Separated       None   \n",
      "3   28  Not-worked  196630  Assoc-voc      11           Separated       None   \n",
      "4   67  Not-worked  150264  Doctorate      16  Married-civ-spouse       None   \n",
      "\n",
      "    relationship   race     sex  gain  loss  hours         native  gross  \n",
      "0      Unmarried  Black  Female     0     0     32  United-States  <=50K  \n",
      "1  Not-in-family  White    Male     0     0     35         Poland  <=50K  \n",
      "2      Unmarried  White  Female     0     0     40         Mexico  <=50K  \n",
      "3      Unmarried  White  Female     0     0     40         Mexico  <=50K  \n",
      "4        Husband  White    Male     0     0     20         Canada   >50K  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('CencusIncome.data.txt')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan encoding pada data categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  fnlwgt  edunum  gain  loss  hours  gross  Federal-gov  Local-gov  \\\n",
      "0   42   51795       9     0     0     32      0            0          0   \n",
      "1   60  251572       9     0     0     35      0            0          0   \n",
      "2   28  201844       9     0     0     40      0            0          0   \n",
      "3   28  196630      11     0     0     40      0            0          0   \n",
      "4   67  150264      16     0     0     20      1            0          0   \n",
      "\n",
      "   Never-worked     ...      Portugal  Puerto-Rico  Scotland  South  Taiwan  \\\n",
      "0             0     ...             0            0         0      0       0   \n",
      "1             0     ...             0            0         0      0       0   \n",
      "2             0     ...             0            0         0      0       0   \n",
      "3             0     ...             0            0         0      0       0   \n",
      "4             0     ...             0            0         0      0       0   \n",
      "\n",
      "   Thailand  Trinadad&Tobago  United-States  Vietnam  Yugoslavia  \n",
      "0         0                0              1        0           0  \n",
      "1         0                0              0        0           0  \n",
      "2         0                0              0        0           0  \n",
      "3         0                0              0        0           0  \n",
      "4         0                0              0        0           0  \n",
      "\n",
      "[5 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "# Digunakan OneHotEncoder pada data categorical selain gross income\n",
    "for column in data.columns.values:\n",
    "    if (data.dtypes[column] == object and column != 'gross'):\n",
    "        enc = pd.get_dummies(data[column])\n",
    "        data = data.drop(column, axis=1)\n",
    "        data = data.join(enc)\n",
    "# Digunakan LabelEncoder pada data gross karena kolom tersebut merupakan target\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['<=50K', '>50K'])\n",
    "enc = le.transform(data['gross'])\n",
    "data['gross'] = enc\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan split antara data dan target untuk difit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatrain = data.drop('gross', axis=1)\n",
    "targettrain = data['gross']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan eksperimen dengan Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805035308566\n",
      "[[2387  135]\n",
      " [ 500  235]]\n",
      "0.796068796069\n",
      "[[2331  127]\n",
      " [ 537  261]]\n",
      "0.792383292383\n",
      "[[2353  122]\n",
      " [ 554  227]]\n",
      "0.789004914005\n",
      "[[2327  130]\n",
      " [ 557  242]]\n",
      "0.777641277641\n",
      "[[2277  148]\n",
      " [ 576  255]]\n",
      "0.785933660934\n",
      "[[2322  133]\n",
      " [ 564  237]]\n",
      "0.815724815725\n",
      "[[2406  112]\n",
      " [ 488  250]]\n",
      "0.793304668305\n",
      "[[2344  116]\n",
      " [ 557  239]]\n",
      "0.808046683047\n",
      "[[2382  112]\n",
      " [ 513  249]]\n",
      "0.788697788698\n",
      "[[2327  129]\n",
      " [ 559  241]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "gnb = GaussianNB()\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        gnb = gnb.fit(X_train, Y_train)\n",
    "        pred = gnb.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan ekperimen dengan Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849247774025\n",
      "[[2357  138]\n",
      " [ 353  409]]\n",
      "0.865171990172\n",
      "[[2336  128]\n",
      " [ 311  481]]\n",
      "0.851658476658\n",
      "[[2331  154]\n",
      " [ 329  442]]\n",
      "0.857493857494\n",
      "[[2329  127]\n",
      " [ 337  463]]\n",
      "0.865786240786\n",
      "[[2343  105]\n",
      " [ 332  476]]\n",
      "0.855343980344\n",
      "[[2355  132]\n",
      " [ 339  430]]\n",
      "0.855343980344\n",
      "[[2346  134]\n",
      " [ 337  439]]\n",
      "0.861486486486\n",
      "[[2362  121]\n",
      " [ 330  443]]\n",
      "0.858415233415\n",
      "[[2312  124]\n",
      " [ 337  483]]\n",
      "0.847972972973\n",
      "[[2276  210]\n",
      " [ 285  485]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "tclf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=10)\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        tclf = tclf.fit(X_train, Y_train)\n",
    "        pred = tclf.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan ekperimen dengan Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795517347252\n",
      "[[2376   96]\n",
      " [ 570  215]]\n",
      "0.785626535627\n",
      "[[2418   51]\n",
      " [ 647  140]]\n",
      "0.791154791155\n",
      "[[2328  135]\n",
      " [ 545  248]]\n",
      "0.681511056511\n",
      "[[1660  816]\n",
      " [ 221  559]]\n",
      "0.789312039312\n",
      "[[2431   37]\n",
      " [ 649  139]]\n",
      "0.803132678133\n",
      "[[2468   34]\n",
      " [ 607  147]]\n",
      "0.764434889435\n",
      "[[2453    7]\n",
      " [ 760   36]]\n",
      "0.799754299754\n",
      "[[2444   37]\n",
      " [ 615  160]]\n",
      "0.779176904177\n",
      "[[2447   18]\n",
      " [ 701   90]]\n",
      "0.773648648649\n",
      "[[2454   10]\n",
      " [ 727   65]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "nclf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(20, 20, 20), random_state=1, max_iter=500)\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        nclf = nclf.fit(X_train, Y_train)\n",
    "        pred = nclf.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773411114523\n",
      "[[2286  228]\n",
      " [ 510  233]]\n",
      "0.772113022113\n",
      "[[2245  220]\n",
      " [ 522  269]]\n",
      "0.778255528256\n",
      "[[2271  201]\n",
      " [ 521  263]]\n",
      "0.766584766585\n",
      "[[2241  224]\n",
      " [ 536  255]]\n",
      "0.779791154791\n",
      "[[2263  173]\n",
      " [ 544  276]]\n",
      "0.769041769042\n",
      "[[2251  205]\n",
      " [ 547  253]]\n",
      "0.777948402948\n",
      "[[2292  215]\n",
      " [ 508  241]]\n",
      "0.789619164619\n",
      "[[2310  193]\n",
      " [ 492  261]]\n",
      "0.781941031941\n",
      "[[2281  187]\n",
      " [ 523  265]]\n",
      "0.769656019656\n",
      "[[2246  188]\n",
      " [ 562  260]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "knn = KNeighborsClassifier()\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        knn = knn.fit(X_train, Y_train)\n",
    "        pred = knn.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami menggunakan algoritma DecisionTreeClassifier karena algoritma tersebut memiliki akurasi yang paling tinggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tclf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=10)\n",
    "tclf = tclf.fit(datatrain, targettrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpan model ke file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tclf, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca data test dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age   workclass  fnlwgt           edu  edunum             marital  \\\n",
      "0   25     Private  226802          11th       7       Never-married   \n",
      "1   38     Private   89814       HS-grad       9  Married-civ-spouse   \n",
      "2   28   Local-gov  336951    Assoc-acdm      12  Married-civ-spouse   \n",
      "3   44     Private  160323  Some-college      10  Married-civ-spouse   \n",
      "4   58  Not-worked  299831       HS-grad       9  Married-civ-spouse   \n",
      "\n",
      "          occupation relationship   race   sex  gain  loss  hours  \\\n",
      "0  Machine-op-inspct    Own-child  Black  Male     0     0     40   \n",
      "1    Farming-fishing      Husband  White  Male     0     0     50   \n",
      "2    Protective-serv      Husband  White  Male     0     0     40   \n",
      "3  Machine-op-inspct      Husband  Black  Male  7688     0     40   \n",
      "4               None      Husband  White  Male     0     0     35   \n",
      "\n",
      "          native  gross  \n",
      "0  United-States  <=50K  \n",
      "1  United-States  <=50K  \n",
      "2  United-States   >50K  \n",
      "3  United-States   >50K  \n",
      "4  United-States  <=50K  \n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('CencusIncome.test.txt')\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan encoding pada data categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  fnlwgt  edunum  gain  loss  hours  gross  Federal-gov  Local-gov  \\\n",
      "0   25  226802       7     0     0     40      0            0          0   \n",
      "1   38   89814       9     0     0     50      0            0          0   \n",
      "2   28  336951      12     0     0     40      1            0          1   \n",
      "3   44  160323      10  7688     0     40      1            0          0   \n",
      "4   58  299831       9     0     0     35      0            0          0   \n",
      "\n",
      "   Never-worked     ...      Portugal  Puerto-Rico  Scotland  South  Taiwan  \\\n",
      "0             0     ...             0            0         0      0       0   \n",
      "1             0     ...             0            0         0      0       0   \n",
      "2             0     ...             0            0         0      0       0   \n",
      "3             0     ...             0            0         0      0       0   \n",
      "4             0     ...             0            0         0      0       0   \n",
      "\n",
      "   Thailand  Trinadad&Tobago  United-States  Vietnam  Yugoslavia  \n",
      "0         0                0              1        0           0  \n",
      "1         0                0              1        0           0  \n",
      "2         0                0              1        0           0  \n",
      "3         0                0              1        0           0  \n",
      "4         0                0              1        0           0  \n",
      "\n",
      "[5 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "# Digunakan OneHotEncoder pada data categorical selain gross income\n",
    "for column in test.columns.values:\n",
    "    if (test.dtypes[column] == object and column != 'gross'):\n",
    "        enc = pd.get_dummies(test[column])\n",
    "        test = test.drop(column, axis=1)\n",
    "        test = test.join(enc)\n",
    "# Digunakan LabelEncoder pada data gross karena kolom tersebut merupakan target\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['<=50K', '>50K'])\n",
    "enc = le.transform(test['gross'])\n",
    "test['gross'] = enc\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menambah kolom yang kurang pada data test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat satu kategori pada native country yang tidak muncul pada data test, yaitu Holand-Netherlands. Akibatnya saat dilakukan OneHotEncoding, kolom tersebut tidak muncul, padahal kolom dari data training dan test harus sama. Maka solusinya adalah menambah kolom Holand-Netherlands yang berisi 0 pada setiap row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Holand-Netherlands'] = 0\n",
    "test = test[data.columns.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan split antara data dan target untuk diprediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatest = test.drop('gross', axis=1)\n",
    "targettest = test['gross']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "0.860205147104\n",
      "[[11821   614]\n",
      " [ 1662  2184]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(datatest)\n",
    "print(pred)\n",
    "print(accuracy_score(targettest, pred))\n",
    "print(confusion_matrix(targettest, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
