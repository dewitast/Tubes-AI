{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar IF3170 Inteligensia Buatan 2017/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplikasi Web Prediksi Income Per Tahun Tahap 1 : Eksperimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anggota :\n",
    "    1. Dewita Sonya Tarabunga - 13515021\n",
    "    2. Helena Suzane Graciella - 13515032\n",
    "    4. Emilia Razak - 13515056\n",
    "    3. Jehian Norman Saviero - 13515139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data training yang diberikan merupakan data dari 32561 hasil sensus yang memberikan data-data berupa age, workclass, final weight, education, education number, marital status, occupation, race, sex, capital gain, capital loss, hours per week, dan native country. Terdapat juga data test yang akan digunakan untuk melakukan test hasil pembelajaran.\n",
    "\n",
    "Analisis yang kami lakukan terhadap data adalah analisis terhadap missing value pada data. Terdapat 3 kolom pada data yang memiliki missing value, yaitu '?' yaitu workclass, occupation, dan native country. Selain itu, semua kolom lain tidak memiliki missing value. Setelah dilihat dari data, semua orang yang memiliki missing value pada occupation hanya memiliki dua kemungkinan nilai workclass, yaitu Never-worked atau missing value. Jelas bahwa jika workclass adalah Never-worked, maka orang tersebut tidak memiliki pekerjaan sehingga value nya kosong. Lalu, jika dilihat dari data yang memiliki missing value di keduanya, hampir 90% dari orang tersebut memiliki gross income <= 50K, maka kami mengasumsikan bahwa orang tersebut pernah bekerja, namun sekarang tidak bekerja sehingga tidak masuk ke kategori manapun dalam workclass. Maka kami mengubah missing value di workclass menjadi Not-worked dan missing value di occupation menjadi None.\n",
    "\n",
    "Satu kolom lagi yang memiliki missing value adalah native country, namun native country merupakan data categorical sehingga tidak masuk akal jika diganti dengan data mean atau median, begitu pula data modus juga kurang cocok untuk menggantikan missing value. Maka kami menganggap missing value sebagai negara lain di luar daftar, atau Other\n",
    "Setelah analisis, kami akan melakukan eksperimen dengan akurasi menjadi ukuran kinerja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperimen : 10-Cross Fold Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca data training dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age   workclass  fnlwgt        edu  edunum             marital occupation  \\\n",
      "0   42  Not-worked   51795    HS-grad       9            Divorced       None   \n",
      "1   60  Not-worked  251572    HS-grad       9             Widowed       None   \n",
      "2   28  Not-worked  201844    HS-grad       9           Separated       None   \n",
      "3   28  Not-worked  196630  Assoc-voc      11           Separated       None   \n",
      "4   67  Not-worked  150264  Doctorate      16  Married-civ-spouse       None   \n",
      "\n",
      "    relationship   race     sex  gain  loss  hours         native  gross  \n",
      "0      Unmarried  Black  Female     0     0     32  United-States  <=50K  \n",
      "1  Not-in-family  White    Male     0     0     35         Poland  <=50K  \n",
      "2      Unmarried  White  Female     0     0     40         Mexico  <=50K  \n",
      "3      Unmarried  White  Female     0     0     40         Mexico  <=50K  \n",
      "4        Husband  White    Male     0     0     20         Canada   >50K  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('CencusIncome.data.txt')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan encoding pada data categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  fnlwgt  edunum  gain  loss  hours  gross  Federal-gov  Local-gov  \\\n",
      "0   42   51795       9     0     0     32      0            0          0   \n",
      "1   60  251572       9     0     0     35      0            0          0   \n",
      "2   28  201844       9     0     0     40      0            0          0   \n",
      "3   28  196630      11     0     0     40      0            0          0   \n",
      "4   67  150264      16     0     0     20      1            0          0   \n",
      "\n",
      "   Never-worked     ...      Portugal  Puerto-Rico  Scotland  South  Taiwan  \\\n",
      "0             0     ...             0            0         0      0       0   \n",
      "1             0     ...             0            0         0      0       0   \n",
      "2             0     ...             0            0         0      0       0   \n",
      "3             0     ...             0            0         0      0       0   \n",
      "4             0     ...             0            0         0      0       0   \n",
      "\n",
      "   Thailand  Trinadad&Tobago  United-States  Vietnam  Yugoslavia  \n",
      "0         0                0              1        0           0  \n",
      "1         0                0              0        0           0  \n",
      "2         0                0              0        0           0  \n",
      "3         0                0              0        0           0  \n",
      "4         0                0              0        0           0  \n",
      "\n",
      "[5 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "# Digunakan OneHotEncoder pada data categorical selain gross income\n",
    "for column in data.columns.values:\n",
    "    if (data.dtypes[column] == object and column != 'gross'):\n",
    "        enc = pd.get_dummies(data[column])\n",
    "        data = data.drop(column, axis=1)\n",
    "        data = data.join(enc)\n",
    "# Digunakan LabelEncoder pada data gross karena kolom tersebut merupakan target\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['<=50K', '>50K'])\n",
    "enc = le.transform(data['gross'])\n",
    "data['gross'] = enc\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan split antara data dan target untuk difit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatrain = data.drop('gross', axis=1)\n",
    "targettrain = data['gross']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan eksperimen dengan Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78999078907\n",
      "[[2330  141]\n",
      " [ 543  243]]\n",
      "0.798832923833\n",
      "[[2356  120]\n",
      " [ 535  245]]\n",
      "0.795147420147\n",
      "[[2366  120]\n",
      " [ 547  223]]\n",
      "0.793304668305\n",
      "[[2335  119]\n",
      " [ 554  248]]\n",
      "0.781326781327\n",
      "[[2301  143]\n",
      " [ 569  243]]\n",
      "0.795454545455\n",
      "[[2355  131]\n",
      " [ 535  235]]\n",
      "0.790540540541\n",
      "[[2327  139]\n",
      " [ 543  247]]\n",
      "0.800061425061\n",
      "[[2346  125]\n",
      " [ 526  259]]\n",
      "0.79699017199\n",
      "[[2355  115]\n",
      " [ 546  240]]\n",
      "0.810503685504\n",
      "[[2383  113]\n",
      " [ 504  256]]\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        gnb = gnb.fit(X_train, Y_train)\n",
    "        pred = gnb.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan ekperimen dengan Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851089960086\n",
      "[[2313  151]\n",
      " [ 334  459]]\n",
      "0.85042997543\n",
      "[[2296  156]\n",
      " [ 331  473]]\n",
      "0.84613022113\n",
      "[[2332  147]\n",
      " [ 354  423]]\n",
      "0.859029484029\n",
      "[[2290  210]\n",
      " [ 249  507]]\n",
      "0.862407862408\n",
      "[[2338  123]\n",
      " [ 325  470]]\n",
      "0.848894348894\n",
      "[[2293  159]\n",
      " [ 333  471]]\n",
      "0.853501228501\n",
      "[[2302  183]\n",
      " [ 294  477]]\n",
      "0.865479115479\n",
      "[[2322  110]\n",
      " [ 328  496]]\n",
      "0.856265356265\n",
      "[[2380  116]\n",
      " [ 352  408]]\n",
      "0.860872235872\n",
      "[[2413   86]\n",
      " [ 367  390]]\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "tclf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=10)\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        tclf = tclf.fit(X_train, Y_train)\n",
    "        pred = tclf.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan ekperimen dengan Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792140006141\n",
      "[[2397   71]\n",
      " [ 606  183]]\n",
      "0.790847665848\n",
      "[[2469   17]\n",
      " [ 664  106]]\n",
      "0.24785012285\n",
      "[[   0 2449]\n",
      " [   0  807]]\n",
      "0.240786240786\n",
      "[[   0 2472]\n",
      " [   0  784]]\n",
      "0.777641277641\n",
      "[[2211  282]\n",
      " [ 442  321]]\n",
      "0.782555282555\n",
      "[[2435   14]\n",
      " [ 694  113]]\n",
      "0.797911547912\n",
      "[[2367  132]\n",
      " [ 526  231]]\n",
      "0.765663390663\n",
      "[[2468    3]\n",
      " [ 760   25]]\n",
      "0.789926289926\n",
      "[[2408   66]\n",
      " [ 618  164]]\n",
      "0.244778869779\n",
      "[[   0 2459]\n",
      " [   0  797]]\n"
     ]
    }
   ],
   "source": [
    "nclf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(20, 20, 20), random_state=1, max_iter=500)\n",
    "for train_index, test_index in kf.split(datatrain):\n",
    "        X_train, X_test = datatrain.iloc[train_index], datatrain.iloc[test_index]\n",
    "        Y_train, Y_test = targettrain.iloc[train_index], targettrain.iloc[test_index]\n",
    "        nclf = nclf.fit(X_train, Y_train)\n",
    "        pred = nclf.predict(X_test)\n",
    "        print(accuracy_score(Y_test, pred))\n",
    "        print(confusion_matrix(Y_test, pred, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tclf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=10)\n",
    "tclf = tclf.fit(datatrain, targettrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpan model ke file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tclf, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca data test dari file eksternal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age   workclass  fnlwgt           edu  edunum             marital  \\\n",
      "0   25     Private  226802          11th       7       Never-married   \n",
      "1   38     Private   89814       HS-grad       9  Married-civ-spouse   \n",
      "2   28   Local-gov  336951    Assoc-acdm      12  Married-civ-spouse   \n",
      "3   44     Private  160323  Some-college      10  Married-civ-spouse   \n",
      "4   58  Not-worked  299831       HS-grad       9  Married-civ-spouse   \n",
      "\n",
      "          occupation relationship   race   sex  gain  loss  hours  \\\n",
      "0  Machine-op-inspct    Own-child  Black  Male     0     0     40   \n",
      "1    Farming-fishing      Husband  White  Male     0     0     50   \n",
      "2    Protective-serv      Husband  White  Male     0     0     40   \n",
      "3  Machine-op-inspct      Husband  Black  Male  7688     0     40   \n",
      "4               None      Husband  White  Male     0     0     35   \n",
      "\n",
      "          native  gross  \n",
      "0  United-States  <=50K  \n",
      "1  United-States  <=50K  \n",
      "2  United-States   >50K  \n",
      "3  United-States   >50K  \n",
      "4  United-States  <=50K  \n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('CencusIncome.test.txt')\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan encoding pada data categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  fnlwgt  edunum  gain  loss  hours  gross  Federal-gov  Local-gov  \\\n",
      "0   25  226802       7     0     0     40      0            0          0   \n",
      "1   38   89814       9     0     0     50      0            0          0   \n",
      "2   28  336951      12     0     0     40      1            0          1   \n",
      "3   44  160323      10  7688     0     40      1            0          0   \n",
      "4   58  299831       9     0     0     35      0            0          0   \n",
      "\n",
      "   Never-worked     ...      Portugal  Puerto-Rico  Scotland  South  Taiwan  \\\n",
      "0             0     ...             0            0         0      0       0   \n",
      "1             0     ...             0            0         0      0       0   \n",
      "2             0     ...             0            0         0      0       0   \n",
      "3             0     ...             0            0         0      0       0   \n",
      "4             0     ...             0            0         0      0       0   \n",
      "\n",
      "   Thailand  Trinadad&Tobago  United-States  Vietnam  Yugoslavia  \n",
      "0         0                0              1        0           0  \n",
      "1         0                0              1        0           0  \n",
      "2         0                0              1        0           0  \n",
      "3         0                0              1        0           0  \n",
      "4         0                0              1        0           0  \n",
      "\n",
      "[5 rows x 108 columns]\n"
     ]
    }
   ],
   "source": [
    "# Digunakan OneHotEncoder pada data categorical selain gross income\n",
    "for column in test.columns.values:\n",
    "    if (test.dtypes[column] == object and column != 'gross'):\n",
    "        enc = pd.get_dummies(test[column])\n",
    "        test = test.drop(column, axis=1)\n",
    "        test = test.join(enc)\n",
    "# Digunakan LabelEncoder pada data gross karena kolom tersebut merupakan target\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['<=50K', '>50K'])\n",
    "enc = le.transform(test['gross'])\n",
    "test['gross'] = enc\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menambah kolom yang kurang pada data test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat satu kategori pada native country yang tidak muncul pada data test, yaitu Holand-Netherlands. Akibatnya saat dilakukan OneHotEncoding, kolom tersebut tidak muncul, padahal kolom dari data training dan test harus sama. Maka solusinya adalah menambah kolom Holand-Netherlands yang berisi 0 pada setiap row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Holand-Netherlands'] = 0\n",
    "test = test[data.columns.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan split antara data dan target untuk diprediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datatest = test.drop('gross', axis=1)\n",
    "targettest = test['gross']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Melakukan prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "0.860020883238\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(datatest)\n",
    "print(pred)\n",
    "print(accuracy_score(targettest, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
